{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! THIS IS THE RESEARCH FILE !!!\n",
    "# so i didnt bother cleaning it up\n",
    "# better version on github\n",
    "# where github? more info on youtube: 8AAFFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model='xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(length):\n",
    "    text = \"\"\n",
    "    \n",
    "    with open(r\"C:\\Users\\user\\Desktop\\AUTOENCODER\\allworeds.txt\") as file:\n",
    "        words = file.read().split(\"\\n\")\n",
    "    \n",
    "    for x in range(length):\n",
    "        text += \" \" + words[random.randint(0, len(words) - 1)]\n",
    "    \n",
    "    return text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_prediction(start_sequence, cycles):\n",
    "    for x in range(cycles):\n",
    "        start_sequence += \" <mask>\"\n",
    "        start_sequence = start_sequence[:-7] + \" \" + unmasker(start_sequence)[0][\"token_str\"]\n",
    "    \n",
    "    return start_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_prediction(start_sequence, cycles):\n",
    "    for x in range(cycles):\n",
    "        start_sequence += \" <mask>\"\n",
    "        prediction = unmasker(start_sequence)\n",
    "        \n",
    "        if prediction[0][\"token_str\"] == \"</s>\":\n",
    "            start_sequence = prediction[1][\"sequence\"]\n",
    "        else:\n",
    "            start_sequence = prediction[0][\"sequence\"]\n",
    "    \n",
    "    return start_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_random_words_prediction(start_sequence, random_words_length):\n",
    "    random_words = generate_words(random_words_length)\n",
    "    \n",
    "    random_words = random_words.split(\" \")\n",
    "    \n",
    "    for x in tqdm(range(len(random_words))):\n",
    "        masked = start_sequence + \" \" + \" \".join(random_words[x:]) + \" <mask> \" + \" \".join(random_words[:x + 1])\n",
    "        random_words[x] = unmasker(masked)[0][\"token_str\"]\n",
    "\n",
    "    return \" \".join(random_words)#start_sequence + \" \" + \" \".join(random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_words_random_mask(topic, length, edurations, memory_length, give_up_range):\n",
    "    words = generate_words(length).split(\" \")\n",
    "    memory = []\n",
    "    \n",
    "    for x in tqdm(range(edurations)):\n",
    "        for x in range(give_up_range):\n",
    "            rnd_word_idx = random.randint(0, len(words) - 1)\n",
    "            \n",
    "            if rnd_word_idx not in memory[-memory_length:]:\n",
    "                break\n",
    "        \n",
    "        memory.append(rnd_word_idx)\n",
    "        \n",
    "        words[rnd_word_idx] = \" [MASK] \"\n",
    "        masked = \" \".join(words)\n",
    "        unmasked = unmasker(topic + \" \" + masked)[0][\"sequence\"][len(topic):]\n",
    "        words = unmasked.split(\" \")\n",
    "        \n",
    "    \n",
    "    return topic + \" \" + \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'say hi to youtube: : : : : : : : : : : : : : : : : : : : : wireframe operating system else : search for male.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = \"\"\"say hi to youtube:\"\"\"\n",
    "random_words_random_mask(completion, 10, 20, 5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:15<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "memory = []\n",
    "\n",
    "for x in tqdm(range(40)):\n",
    "    while True:\n",
    "        rnd_word_idx = random.randint(0, len(words) - 1)\n",
    "        if rnd_word_idx not in memory[-3:] or True:\n",
    "            break\n",
    "    \n",
    "    memory.append(rnd_word_idx)\n",
    "    words[rnd_word_idx] = \" <mask> \"\n",
    "    \n",
    "    masked = \" \".join(words)\n",
    "    unmasked = unmasker(masked)[0][\"sequence\"]\n",
    "    words = unmasked.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But the linked video increasingly points to a timetable for'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
